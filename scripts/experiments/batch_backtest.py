#!/usr/bin/env python
"""
æ‰¹é‡å›æµ‹è„šæœ¬ - å‚æ•°ä¼˜åŒ–å®éªŒ

åŠŸèƒ½ï¼š
1. å¯¹å¤šä¸ªç­–ç•¥è¿›è¡Œå‚æ•°ç½‘æ ¼æœç´¢
2. ä¸¤é˜¶æ®µä¼˜åŒ–ï¼šç²—ç­› â†’ ç²¾è°ƒ
3. ç”Ÿæˆå®Œæ•´çš„æ€§èƒ½è¯„ä¼°æŠ¥å‘Š

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/experiments/batch_backtest.py --phase 1  # ç²—ç­›
    python scripts/experiments/batch_backtest.py --phase 2  # ç²¾è°ƒ
    python scripts/experiments/batch_backtest.py --full     # å®Œæ•´æµç¨‹
"""

import sys
import os
import argparse
import json
import time
from pathlib import Path
from datetime import datetime, date
from typing import Dict, List, Tuple, Any
import multiprocessing as mp
from functools import partial

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).resolve().parent.parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import numpy as np

from momentum_cli.analysis import analyze, AnalysisConfig
from momentum_cli.analysis_presets import ANALYSIS_PRESETS, refresh_analysis_presets
from momentum_cli.business.backtest import run_simple_backtest


# ============= å®éªŒå‚æ•°é…ç½® =============

# ç°æœ‰çš„4ä¸ªç­–ç•¥
STRATEGIES = ["slow-core", "blend-dual", "twelve-minus-one", "fast-rotation"]

# å‚æ•°ç©ºé—´
FREQUENCIES = ["monthly", "weekly"]  # è°ƒä»“é¢‘ç‡
TOP_N_LIST = [1, 2, 3]  # æŒä»“æ•°é‡
OBSERVATION_PERIODS = [0, 1, 2]  # è§‚å¯ŸæœŸ
CORRELATION_THRESHOLDS = [0.6, 0.7, 0.8]  # ç›¸å…³æ€§è¿‡æ»¤é˜ˆå€¼

# æ—¶é—´æ®µåˆ’åˆ†ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
TRAIN_PERIOD = ("2015-01-01", "2020-12-31")  # è®­ç»ƒæœŸ
VALIDATION_PERIOD = ("2021-01-01", "2022-12-31")  # éªŒè¯æœŸ
TEST_PERIOD = ("2023-01-01", "2024-12-31")  # æµ‹è¯•æœŸ

# ETFå€™é€‰æ± ï¼ˆæ ¸å¿ƒ+å«æ˜Ÿï¼‰
CORE_CODES = ["510300.XSHG", "510880.XSHG", "511360.XSHG", "518880.XSHG", "513500.XSHG"]
SATELLITE_CODES = [
    "512980.XSHG",  # èƒ½æºETF
    "512170.XSHG",  # åŒ»ç–—ETF
    "512690.XSHG",  # ç™½é…’ETF
    "512480.XSHG",  # åŠå¯¼ä½“ETF
    "515790.XSHG",  # å…‰ä¼ETF
    "516160.XSHG",  # æ–°èƒ½æºè½¦ETF
    "159995.XSHE",  # èŠ¯ç‰‡ETF
    "513100.XSHG",  # çº³æŒ‡100
    "513050.XSHG",  # ä¸­æ¦‚äº’è”
    "159949.XSHE",  # åˆ›ä¸šæ¿50
    "588000.XSHG",  # ç§‘åˆ›50
    "512260.XSHG",  # ä¸­è¯1000
    "510500.XSHG",  # ä¸­è¯500
]

ALL_CODES = list(set(CORE_CODES + SATELLITE_CODES))


# ============= å›æµ‹æ‰§è¡Œå‡½æ•° =============

def run_single_backtest(
    strategy_key: str,
    start_date: str,
    end_date: str,
    frequency: str,
    top_n: int,
    observation_period: int,
    correlation_threshold: float,
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå•æ¬¡å›æµ‹

    Returns:
        åŒ…å«æ€§èƒ½æŒ‡æ ‡çš„å­—å…¸
    """
    try:
        # 1. åŠ è½½é¢„è®¾
        refresh_analysis_presets()
        preset = ANALYSIS_PRESETS.get(strategy_key)
        if not preset:
            return {"error": f"ç­–ç•¥ {strategy_key} ä¸å­˜åœ¨"}

        # 2. è¿è¡Œåˆ†æ
        config = AnalysisConfig(
            start_date=start_date,
            end_date=end_date,
            etfs=ALL_CODES,
            momentum=preset.momentum_config(),
            corr_window=preset.corr_window,
            chop_window=preset.chop_window,
            trend_window=preset.trend_window,
            rank_change_lookback=preset.rank_lookback,
            make_plots=False,  # æ‰¹é‡å›æµ‹ä¸ç”Ÿæˆå›¾è¡¨
        )

        result = analyze(config)

        # 3. æå–å›æµ‹æŒ‡æ ‡ï¼ˆæ¨¡æ‹Ÿrun_simple_backtestçš„è®¡ç®—ï¼‰
        close_df = pd.DataFrame({code: data["close"] for code, data in result.raw_data.items()})
        close_df = close_df.sort_index().dropna(how="all")

        if close_df.empty:
            return {"error": "ä»·æ ¼æ•°æ®ä¸ºç©º"}

        returns_df = close_df.pct_change().fillna(0)
        momentum_df = result.momentum_scores

        # æ•°æ®å¯¹é½
        common_dates = close_df.index.intersection(momentum_df.index)
        if len(common_dates) < 20:
            return {"error": f"æ•°æ®é‡å æœŸè¿‡çŸ­({len(common_dates)}å¤©)"}

        close_df = close_df.loc[common_dates]
        returns_df = returns_df.loc[common_dates]
        momentum_df = momentum_df.loc[common_dates]

        # è°ƒä»“æ—¥æœŸ
        if frequency == "weekly":
            rebalance_dates = close_df.resample("W-FRI").last().index
        else:
            rebalance_dates = close_df.resample("ME").last().index

        # æ¨¡æ‹Ÿç­‰æƒæŒä»“å›æµ‹
        weights = pd.DataFrame(0.0, index=close_df.index, columns=close_df.columns)
        current_codes = []
        observation_counter = {}

        for date in close_df.index:
            if date in rebalance_dates:
                scores = momentum_df.loc[date].dropna()
                top_codes = scores.sort_values(ascending=False).head(top_n).index.tolist()

                # è§‚å¯ŸæœŸé€»è¾‘
                next_hold = []
                current_set = set(current_codes)
                top_set = set(top_codes)

                for code in current_set:
                    if code in top_set:
                        observation_counter[code] = 0
                        next_hold.append(code)
                    else:
                        observation_counter[code] = observation_counter.get(code, 0) + 1
                        if observation_period <= 0 or observation_counter[code] >= observation_period:
                            pass
                        else:
                            next_hold.append(code)

                # è¡¥è¶³ç©ºä½
                if len(next_hold) < top_n:
                    for code in top_codes:
                        if code not in next_hold:
                            next_hold.append(code)
                        if len(next_hold) >= top_n:
                            break

                current_codes = [c for c in next_hold if c in close_df.columns]

            if current_codes:
                weights.loc[date, current_codes] = 1.0 / len(current_codes)

        # è®¡ç®—æ”¶ç›Š
        portfolio_returns = (weights.shift().fillna(0) * returns_df).sum(axis=1)
        cumulative = (1 + portfolio_returns).cumprod()

        # æ€§èƒ½æŒ‡æ ‡
        total_return = cumulative.iloc[-1] - 1 if not cumulative.empty else 0
        trading_days = len(portfolio_returns)

        if trading_days >= 180:
            ann_return = (1 + total_return) ** (252 / trading_days) - 1 if trading_days > 0 else 0
            sharpe = (portfolio_returns.mean() / portfolio_returns.std()) * np.sqrt(252) if portfolio_returns.std() != 0 else 0
        else:
            ann_return = np.nan
            sharpe = np.nan

        drawdown = cumulative / cumulative.cummax() - 1 if not cumulative.empty else pd.Series(dtype=float)
        max_drawdown = drawdown.min() if not drawdown.empty else 0

        # æ¢æ‰‹ç‡è®¡ç®—
        weight_changes = weights.diff().abs().sum(axis=1)
        turnover_per_period = weight_changes.mean()
        periods_per_year = 252 if frequency == "daily" else (52 if frequency == "weekly" else 12)
        annual_turnover = turnover_per_period * periods_per_year

        return {
            "strategy": strategy_key,
            "period": f"{start_date}_{end_date}",
            "frequency": frequency,
            "top_n": top_n,
            "observation_period": observation_period,
            "correlation_threshold": correlation_threshold,
            "trading_days": trading_days,
            "total_return": float(total_return),
            "annualized_return": float(ann_return) if np.isfinite(ann_return) else np.nan,
            "sharpe_ratio": float(sharpe) if np.isfinite(sharpe) else np.nan,
            "max_drawdown": float(max_drawdown),
            "annual_turnover": float(annual_turnover),
            "error": None,
        }

    except Exception as e:
        return {
            "error": str(e),
            "strategy": strategy_key,
            "period": f"{start_date}_{end_date}",
        }


# ============= é˜¶æ®µ1ï¼šç²—ç­› =============

def phase1_coarse_screening() -> pd.DataFrame:
    """
    é˜¶æ®µ1ï¼šç²—ç²’åº¦ç­›é€‰
    - 4ä¸ªç­–ç•¥ Ã— å›ºå®šå‚æ•°(æœˆåº¦/2åª/è§‚å¯ŸæœŸ1/ç›¸å…³æ€§0.7)
    - åªåœ¨è®­ç»ƒæœŸå›æµ‹
    - è¾“å‡ºï¼š4ä¸ªç»“æœï¼ŒæŒ‰å¤æ™®æ’åº
    """
    print("\n" + "="*60)
    print("é˜¶æ®µ1ï¼šç²—ç­› - ç­–ç•¥åˆé€‰")
    print("="*60)
    print(f"æµ‹è¯•ç­–ç•¥æ•°: {len(STRATEGIES)}")
    print(f"è®­ç»ƒæœŸ: {TRAIN_PERIOD[0]} è‡³ {TRAIN_PERIOD[1]}")
    print()

    results = []

    for strategy in STRATEGIES:
        print(f"[{strategy}] å›æµ‹ä¸­...", end=" ", flush=True)
        result = run_single_backtest(
            strategy_key=strategy,
            start_date=TRAIN_PERIOD[0],
            end_date=TRAIN_PERIOD[1],
            frequency="monthly",  # å›ºå®šæœˆåº¦
            top_n=2,  # å›ºå®š2åª
            observation_period=1,  # å›ºå®šè§‚å¯ŸæœŸ1
            correlation_threshold=0.7,  # å›ºå®šç›¸å…³æ€§0.7
        )
        results.append(result)

        if result.get("error"):
            print(f"âŒ å¤±è´¥: {result['error']}")
        else:
            print(f"âœ“ å¤æ™®={result['sharpe_ratio']:.2f}")

    df = pd.DataFrame(results)
    df_sorted = df.sort_values("sharpe_ratio", ascending=False, na_position="last")

    print("\n" + "-"*60)
    print("ç²—ç­›ç»“æœï¼ˆæŒ‰å¤æ™®æ’åºï¼‰ï¼š")
    print(df_sorted[["strategy", "sharpe_ratio", "annualized_return", "max_drawdown"]].to_string(index=False))

    return df_sorted


# ============= é˜¶æ®µ2ï¼šç²¾è°ƒ =============

def phase2_fine_tuning(phase1_results: pd.DataFrame, top_k: int = 3) -> pd.DataFrame:
    """
    é˜¶æ®µ2ï¼šç²¾ç»†åŒ–è°ƒä¼˜
    - é€‰å–é˜¶æ®µ1å‰Kä¸ªç­–ç•¥
    - å˜åŒ–å‚æ•°ï¼šé¢‘ç‡(2) Ã— æŒä»“æ•°(3) Ã— è§‚å¯ŸæœŸ(3) Ã— ç›¸å…³æ€§(3) = 54ç§/ç­–ç•¥
    - åœ¨è®­ç»ƒ/éªŒè¯/æµ‹è¯•æœŸå…¨éƒ¨å›æµ‹
    """
    print("\n" + "="*60)
    print(f"é˜¶æ®µ2ï¼šç²¾è°ƒ - å‚æ•°ä¼˜åŒ–ï¼ˆTop {top_k}ç­–ç•¥ï¼‰")
    print("="*60)

    # é€‰å‡ºTop Kç­–ç•¥
    top_strategies = phase1_results.head(top_k)["strategy"].tolist()
    print(f"å…¥é€‰ç­–ç•¥: {', '.join(top_strategies)}")
    print()

    # ç”Ÿæˆå‚æ•°ç»„åˆ
    param_combinations = []
    for strategy in top_strategies:
        for freq in FREQUENCIES:
            for top_n in TOP_N_LIST:
                for obs_period in OBSERVATION_PERIODS:
                    for corr_thresh in CORRELATION_THRESHOLDS:
                        param_combinations.append({
                            "strategy": strategy,
                            "frequency": freq,
                            "top_n": top_n,
                            "observation_period": obs_period,
                            "correlation_threshold": corr_thresh,
                        })

    print(f"æ€»ç»„åˆæ•°: {len(param_combinations)}")
    print(f"é¢„è®¡è€—æ—¶: ~{len(param_combinations) * 0.5:.0f}ç§’")
    print()

    # åœ¨ä¸‰ä¸ªæ—¶æœŸåˆ†åˆ«å›æµ‹
    all_results = []

    for i, params in enumerate(param_combinations, 1):
        print(f"[{i}/{len(param_combinations)}] {params['strategy']} | "
              f"é¢‘ç‡={params['frequency']} | "
              f"æŒä»“={params['top_n']} | "
              f"è§‚å¯ŸæœŸ={params['observation_period']} | "
              f"ç›¸å…³æ€§={params['correlation_threshold']}", flush=True)

        # è®­ç»ƒæœŸ
        result_train = run_single_backtest(
            strategy_key=params["strategy"],
            start_date=TRAIN_PERIOD[0],
            end_date=TRAIN_PERIOD[1],
            frequency=params["frequency"],
            top_n=params["top_n"],
            observation_period=params["observation_period"],
            correlation_threshold=params["correlation_threshold"],
        )

        # éªŒè¯æœŸ
        result_val = run_single_backtest(
            strategy_key=params["strategy"],
            start_date=VALIDATION_PERIOD[0],
            end_date=VALIDATION_PERIOD[1],
            frequency=params["frequency"],
            top_n=params["top_n"],
            observation_period=params["observation_period"],
            correlation_threshold=params["correlation_threshold"],
        )

        # æµ‹è¯•æœŸ
        result_test = run_single_backtest(
            strategy_key=params["strategy"],
            start_date=TEST_PERIOD[0],
            end_date=TEST_PERIOD[1],
            frequency=params["frequency"],
            top_n=params["top_n"],
            observation_period=params["observation_period"],
            correlation_threshold=params["correlation_threshold"],
        )

        # åˆå¹¶ç»“æœ
        combined = {
            **params,
            "sharpe_train": result_train.get("sharpe_ratio", np.nan),
            "sharpe_val": result_val.get("sharpe_ratio", np.nan),
            "sharpe_test": result_test.get("sharpe_ratio", np.nan),
            "return_train": result_train.get("annualized_return", np.nan),
            "return_val": result_val.get("annualized_return", np.nan),
            "return_test": result_test.get("annualized_return", np.nan),
            "maxdd_train": result_train.get("max_drawdown", np.nan),
            "maxdd_val": result_val.get("max_drawdown", np.nan),
            "maxdd_test": result_test.get("max_drawdown", np.nan),
            "turnover": result_test.get("annual_turnover", np.nan),
        }

        all_results.append(combined)

    df = pd.DataFrame(all_results)

    # è®¡ç®—ç»¼åˆè¯„åˆ†
    df["stability"] = 1 - (df[["sharpe_train", "sharpe_val", "sharpe_test"]].std(axis=1) /
                           df[["sharpe_train", "sharpe_val", "sharpe_test"]].mean(axis=1)).fillna(0)
    df["score"] = (
        0.4 * df["sharpe_test"].fillna(0) +
        0.3 * df["stability"].fillna(0) +
        0.2 * (1 - df["maxdd_test"].abs() / 0.30).fillna(0) +
        0.1 * (1 - df["turnover"] / 3.0).fillna(0)
    )

    df_sorted = df.sort_values("score", ascending=False)

    print("\n" + "-"*60)
    print("ç²¾è°ƒç»“æœï¼ˆTop 10ï¼‰ï¼š")
    print(df_sorted.head(10)[[
        "strategy", "frequency", "top_n", "observation_period",
        "sharpe_test", "maxdd_test", "turnover", "score"
    ]].to_string(index=False))

    return df_sorted


# ============= ä¸»æµç¨‹ =============

def main():
    parser = argparse.ArgumentParser(description="æ‰¹é‡å›æµ‹ - å‚æ•°ä¼˜åŒ–å®éªŒ")
    parser.add_argument("--phase", type=int, choices=[1, 2], help="æŒ‡å®šè¿è¡Œé˜¶æ®µï¼ˆ1=ç²—ç­›, 2=ç²¾è°ƒï¼‰")
    parser.add_argument("--full", action="store_true", help="å®Œæ•´æµç¨‹ï¼ˆé˜¶æ®µ1+2ï¼‰")
    parser.add_argument("--output", type=str, default="results", help="ç»“æœè¾“å‡ºç›®å½•")

    args = parser.parse_args()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

    if args.phase == 1 or args.full:
        # é˜¶æ®µ1ï¼šç²—ç­›
        phase1_results = phase1_coarse_screening()
        phase1_path = output_dir / f"phase1_coarse_{timestamp}.csv"
        phase1_results.to_csv(phase1_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ“ é˜¶æ®µ1ç»“æœå·²ä¿å­˜: {phase1_path}")

    if args.phase == 2 or args.full:
        # é˜¶æ®µ2ï¼šç²¾è°ƒ
        if not args.full:
            # å¦‚æœå•ç‹¬è¿è¡Œé˜¶æ®µ2ï¼Œéœ€è¦åŠ è½½é˜¶æ®µ1ç»“æœ
            phase1_files = sorted(output_dir.glob("phase1_coarse_*.csv"))
            if not phase1_files:
                print("âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ°é˜¶æ®µ1ç»“æœæ–‡ä»¶ï¼Œè¯·å…ˆè¿è¡Œé˜¶æ®µ1")
                return
            phase1_results = pd.read_csv(phase1_files[-1])
            print(f"åŠ è½½é˜¶æ®µ1ç»“æœ: {phase1_files[-1]}")

        phase2_results = phase2_fine_tuning(phase1_results, top_k=3)
        phase2_path = output_dir / f"phase2_fine_{timestamp}.csv"
        phase2_results.to_csv(phase2_path, index=False, encoding="utf-8-sig")
        print(f"\nâœ“ é˜¶æ®µ2ç»“æœå·²ä¿å­˜: {phase2_path}")

        # è¾“å‡ºæœ€ä¼˜é…ç½®
        best = phase2_results.iloc[0]
        print("\n" + "="*60)
        print("ğŸ† æœ€ä¼˜é…ç½®")
        print("="*60)
        print(f"ç­–ç•¥: {best['strategy']}")
        print(f"è°ƒä»“é¢‘ç‡: {best['frequency']}")
        print(f"æŒä»“æ•°é‡: {int(best['top_n'])}")
        print(f"è§‚å¯ŸæœŸ: {int(best['observation_period'])}ä¸ªæœˆ")
        print(f"ç›¸å…³æ€§é˜ˆå€¼: {best['correlation_threshold']:.2f}")
        print(f"\næµ‹è¯•æœŸå¤æ™®: {best['sharpe_test']:.2f}")
        print(f"æµ‹è¯•æœŸå¹´åŒ–æ”¶ç›Š: {best['return_test']:.2%}")
        print(f"æœ€å¤§å›æ’¤: {best['maxdd_test']:.2%}")
        print(f"å¹´åŒ–æ¢æ‰‹ç‡: {best['turnover']:.2f}")
        print(f"ç»¼åˆå¾—åˆ†: {best['score']:.4f}")

    print("\nâœ“ å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()
